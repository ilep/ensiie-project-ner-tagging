{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "generate integer-indexed sentences, pos-tags and named entity tags, dictionaries for converting, etc, and save as `npy` binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sujiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import Input, LSTM, Dropout, Embedding,Dense\n",
    "from keras_contrib.layers import CRF\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_ad_to_dataframe(line, nb_line):\n",
    "    \"\"\"Fonction qui à partir d'une ligne d'un fichier Json de créer une dataframe à \n",
    "    trois colonnes. La première colonne correspond au numéro de l'annonce, la seconde \n",
    "    contient les mots de l'annonce et la troisième les positions du mot dans l'annonce.\"\"\" \n",
    "    Vect_word=(word_tokenize(eval(line.strip().replace('\\xa0',' '))[\"text\"])) # Tokenisation\n",
    "    nb_sent_list=list(map(int, nb_line*np.ones(len(Vect_word)))) # Numéro annonce\n",
    "    # Position\n",
    "    offset = 0                                                                  \n",
    "    list_pos=list()\n",
    "    for token in Vect_word:\n",
    "        offset = eval(line.strip().replace('\\xa0',' '))[\"text\"].find(token, offset)\n",
    "        list_pos.append([offset, offset+len(token)])\n",
    "        offset += len(token)\n",
    "    # Creation de la dataframe\n",
    "    data={'Ad#':nb_sent_list,'Words':Vect_word,'Pos':list_pos}\n",
    "    df=pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Fonction qui permet de corriger les annotations qui surlignent un espace blanc\"\"\"\n",
    "    if text[0]==' ':      \n",
    "        if text[-1]==' ':\n",
    "            return 3\n",
    "        return 1\n",
    "    elif text[-1]==' ':\n",
    "        return 2\n",
    "    else :\n",
    "        return 0\n",
    "    \n",
    "def from_line_to_list_label(line):\n",
    "    \"\"\"Fonction qui permet de sortir les informations des labels (text, label et positions)\n",
    "    à partir d'une ligne du fichier json\"\"\"\n",
    "    list_word_label=list()\n",
    "    for i in range(len(eval(line)[\"labels\"])):\n",
    "        start=eval(line)[\"labels\"][i][0]\n",
    "        end=eval(line)[\"labels\"][i][1]\n",
    "        label=eval(line)[\"labels\"][i][2]\n",
    "        if (clean_text(eval(line.strip().replace('\\xa0',' '))[\"text\"][start:end])==0):\n",
    "            list_word_label.append([eval(line.strip().replace('\\xa0',' '))[\"text\"][start:end],label,start,end])\n",
    "        elif (clean_text(eval(line.strip().replace('\\xa0',' '))[\"text\"][start:end])==1):\n",
    "            list_word_label.append([eval(line.strip().replace('\\xa0',' '))[\"text\"][(start+1):end],label,start+1,end])\n",
    "        elif (clean_text(eval(line.strip().replace('\\xa0',' '))[\"text\"][start:end])==2):\n",
    "            list_word_label.append([eval(line.strip().replace('\\xa0',' '))[\"text\"][start:(end-1)],label,start,end-1])\n",
    "        else:\n",
    "            list_word_label.append([eval(line.strip().replace('\\xa0',' '))[\"text\"][(start+1):(end-1)],label,start+1,end-1])\n",
    "    return list_word_label\n",
    "\n",
    "def column_tag(vect_word,list_word_pos_label):\n",
    "    \"\"\"Colonne contenant les labels pour chaque mot d'une annonce\"\"\"\n",
    "    list_tag=[\"O\"]*len(vect_word[\"Pos\"])\n",
    "    for i in range(len(vect_word[\"Pos\"])):\n",
    "        for elmt in list_word_pos_label:\n",
    "            if vect_word[\"Pos\"][i][0]==elmt[2] and vect_word[\"Pos\"][i][1]<=elmt[3]:\n",
    "                list_tag[i]=\"B-\"+elmt[1]\n",
    "            elif vect_word[\"Pos\"][i][0]>elmt[2] and vect_word[\"Pos\"][i][1]<=elmt[3]:\n",
    "                list_tag[i]=\"I-\"+elmt[1]\n",
    "    return list_tag\n",
    "\n",
    "# Lecture du fichier Json\n",
    "cnt = 1 # Numéro annonce\n",
    "for i in range(1,6):\n",
    "    with open('data\\\\doccano\\\\bdd'+str(i)+'.json1', encoding=\"utf-8\") as fp:\n",
    "        line = fp.readline()\n",
    "        if i==1:\n",
    "            df=from_ad_to_dataframe(line.replace('null','\"null\"'),cnt)\n",
    "            list_word_pos_label=from_line_to_list_label(line.replace('null','\"null\"'))\n",
    "            list_tag=column_tag(df,list_word_pos_label)\n",
    "            df[\"Tag\"]=list_tag\n",
    "        else :\n",
    "            df_ad=from_ad_to_dataframe(line.replace('null','\"null\"'),cnt)\n",
    "            list_word_pos_label=from_line_to_list_label(line.replace('null','\"null\"'))\n",
    "            list_tag=column_tag(df_ad,list_word_pos_label)\n",
    "            df_ad[\"Tag\"]=list_tag\n",
    "            df=df.append(df_ad, ignore_index = True)\n",
    "        while line:\n",
    "           #print(\"Line {}: {}\".format(cnt, line.strip()))\n",
    "            if cnt!=1:\n",
    "                df_ad=from_ad_to_dataframe(line.replace('null','\"null\"'),cnt)\n",
    "                list_word_pos_label=from_line_to_list_label(line.replace('null','\"null\"'))\n",
    "                list_tag=column_tag(df_ad,list_word_pos_label)\n",
    "                df_ad[\"Tag\"]=list_tag\n",
    "                df=df.append(df_ad, ignore_index = True)\n",
    "            line = fp.readline()\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ad#</th>\n",
       "      <th>Words</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Situé</td>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>à</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>stations</td>\n",
       "      <td>[10, 18]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>[19, 21]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50303</td>\n",
       "      <td>587</td>\n",
       "      <td>inclus</td>\n",
       "      <td>[187, 193]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50304</td>\n",
       "      <td>587</td>\n",
       "      <td>dans</td>\n",
       "      <td>[194, 198]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50305</td>\n",
       "      <td>587</td>\n",
       "      <td>le</td>\n",
       "      <td>[199, 201]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50306</td>\n",
       "      <td>587</td>\n",
       "      <td>loyer</td>\n",
       "      <td>[202, 207]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50307</td>\n",
       "      <td>587</td>\n",
       "      <td>.</td>\n",
       "      <td>[207, 208]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50308 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Ad#     Words         Pos Tag\n",
       "0        1     Situé      [0, 5]   O\n",
       "1        1         à      [6, 7]   O\n",
       "2        1         6      [8, 9]   O\n",
       "3        1  stations    [10, 18]   O\n",
       "4        1        de    [19, 21]   O\n",
       "...    ...       ...         ...  ..\n",
       "50303  587    inclus  [187, 193]   O\n",
       "50304  587      dans  [194, 198]   O\n",
       "50305  587        le  [199, 201]   O\n",
       "50306  587     loyer  [202, 207]   O\n",
       "50307  587         .  [207, 208]   O\n",
       "\n",
       "[50308 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get vocab, maxvocab\n",
    "# takes sents : list (tokenized lists of sentences)\n",
    "# takes maxvocab : int (maximum vocab size incl. UNK, PAD\n",
    "# takes stoplist : list (words to ignore)\n",
    "# returns vocab_dict (word to index), inv_vocab_dict (index to word)\n",
    "def get_vocab(sent_toks, maxvocab=10000, min_count=1, stoplist=[], unk='UNK', pad='PAD', verbose=True):\n",
    "    # get vocab list\n",
    "    vocab = [word for sent in sent_toks for word in sent]\n",
    "    sorted_vocab = sorted(Counter(vocab).most_common(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_vocab = [i for i in sorted_vocab if i[0] not in stoplist and i[0] != unk]\n",
    "    if verbose:\n",
    "        print(\"total vocab:\", len(sorted_vocab))\n",
    "    sorted_vocab = [i for i in sorted_vocab if i[1] >= min_count]\n",
    "    # reserve for PAD and UNK\n",
    "    sorted_vocab = [i[0] for i in sorted_vocab[:maxvocab - 2]]\n",
    "    vocab_dict = {k: v + 1 for v, k in enumerate(sorted_vocab)}\n",
    "    vocab_dict[unk] = len(sorted_vocab) + 1\n",
    "    vocab_dict[pad] = 0\n",
    "    inv_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_dict, inv_vocab_dict\n",
    "\n",
    "\n",
    "# function to convert sents to indexed vectors\n",
    "# takes list : sents (tokenized sentences)\n",
    "# takes dict : vocab (word to idx mapping)\n",
    "# returns list of lists of indexed sentences\n",
    "def index_sents(sent_tokens, vocab_dict, reverse=False, unk_name='UNK', verbose=False):\n",
    "    vectors = []\n",
    "    for sent in sent_tokens:\n",
    "        sent_vect = []\n",
    "        if reverse:\n",
    "            sent = sent[::-1]\n",
    "        for word in sent:\n",
    "            if word in vocab_dict.keys():\n",
    "                sent_vect.append(vocab_dict[word])\n",
    "            else:  # out of max_vocab range or OOV\n",
    "                sent_vect.append(vocab_dict[unk_name])\n",
    "        vectors.append(np.asarray(sent_vect))\n",
    "    vectors = np.asarray(vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum network vocabulary, test set size\n",
    "MAX_VOCAB = 5000\n",
    "TEST_SIZE = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read ConLL2002 NER corpus from csv (first save as utf-8!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ad#</th>\n",
       "      <th>Words</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Situé</td>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>à</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>stations</td>\n",
       "      <td>[10, 18]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>[19, 21]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50303</td>\n",
       "      <td>587</td>\n",
       "      <td>inclus</td>\n",
       "      <td>[187, 193]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50304</td>\n",
       "      <td>587</td>\n",
       "      <td>dans</td>\n",
       "      <td>[194, 198]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50305</td>\n",
       "      <td>587</td>\n",
       "      <td>le</td>\n",
       "      <td>[199, 201]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50306</td>\n",
       "      <td>587</td>\n",
       "      <td>loyer</td>\n",
       "      <td>[202, 207]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50307</td>\n",
       "      <td>587</td>\n",
       "      <td>.</td>\n",
       "      <td>[207, 208]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50308 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Ad#     Words         Pos Tag\n",
       "0        1     Situé      [0, 5]   O\n",
       "1        1         à      [6, 7]   O\n",
       "2        1         6      [8, 9]   O\n",
       "3        1  stations    [10, 18]   O\n",
       "4        1        de    [19, 21]   O\n",
       "...    ...       ...         ...  ..\n",
       "50303  587    inclus  [187, 193]   O\n",
       "50304  587      dans  [194, 198]   O\n",
       "50305  587        le  [199, 201]   O\n",
       "50306  587     loyer  [202, 207]   O\n",
       "50307  587         .  [207, 208]   O\n",
       "\n",
       "[50308 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '3',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '6',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '10',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '12',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '13',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '14',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '15',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " '16',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentmarks = data[\"Ad#\"].tolist()\n",
    "sentmarks = [str(s) for s in sentmarks]\n",
    "sentmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data[\"Words\"].tolist()\n",
    "nertags = data[\"Tag\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text = []\n",
    "sentence_ners = []\n",
    "\n",
    "vocab = []\n",
    "\n",
    "this_snt = []\n",
    "this_ner = []\n",
    "\n",
    "for idx, s in enumerate(sentmarks):\n",
    "    # reset if new sent\n",
    "    if int(s) == int(sentmarks[idx-1])+1 or idx==0 or idx==len(data)-1:\n",
    "        # edit: ONLY IF HAS TAG!\n",
    "        if len(this_snt) > 0 and (int(s) == int(sentmarks[idx-1])+1 or idx==len(data)-1):\n",
    "            if list(set(this_ner)) != ['O']:\n",
    "                if idx==len(data)-1:\n",
    "                    this_snt.append(words[idx].lower())\n",
    "                    this_ner.append(nertags[idx])\n",
    "                sentence_text.append(this_snt[:-1])\n",
    "                sentence_ners.append(this_ner[:-1])\n",
    "        this_snt = []\n",
    "        this_ner = []\n",
    "    \n",
    "    # add to lists \n",
    "    this_snt.append(words[idx].lower())\n",
    "    this_ner.append(nertags[idx])\n",
    "    vocab.append(words[idx].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['situé', 'à', '6', 'stations', 'de', 'la', 'porte', 'de', 'choisy', 'par', 'le', 'bus', '183', ',', 'station', 'malassis', ',', 'futur', 'tramway', '9', ',', 'à', 'proximité', 'des', 'commerces', ',', 'dans', 'une', 'résidence', 'avec', 'espace', 'vert', ',', 'gardien', ',', 'digicode', ',', 'interphone', ',', 'un', 'appartement', 'de', '3', 'pièces', ',', 'au', '3ème', 'étage', ',', 'comprenant', ':', 'entrée', ',', 'séjour', ',', '2', 'chambres', ',', 'cuisine', ',', 'salle', 'de', 'bains', ',', 'dégagement', 'et', 'wc', '.', 'un', 'parking', 'et', 'une', 'cave', 'au', 'sous-sol', '-', 'chauffage', 'et', 'eau', 'chaude', 'collectif']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TRANSPORTS_PROXIMITE', 'I-TRANSPORTS_PROXIMITE', 'O', 'O', 'O', 'O', 'O', 'B-TRANSPORTS_PROXIMITE', 'I-TRANSPORTS_PROXIMITE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-N_PIECES', 'O', 'O', 'O', 'B-N_ETAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-N_CHAMBRES', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PARKING', 'O', 'O', 'B-STOCKAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TYPE_CHAUFFAGE']\n",
      "\n",
      "['entre', 'mairie', 'et', 'église', ',', 'au', 'pied', 'de', 'toutes', 'commodités', ',', \"l'agence\", 'guy', 'hoquet', 'vous', 'propose', 'ce', 'bel', 'appartement', 'de', '4', 'pièces', 'composé', \"d'une\", 'entrée', 'avec', 'placards', ',', 'balcon', 'donnant', 'sur', 'séjour', ',', 'cuisine', 'aménagée', 'et', 'équipée', ',', '3', 'chambres', ',', 'salle', \"d'eau\", ',', 'wc', ',', 'nombreux', 'rangements', ',', 'parking', 'sous-sol', ',', 'colocation', 'possible']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-N_PIECES', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EXTERIEUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-N_CHAMBRES', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PARKING', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(sentence_text[:2]):\n",
    "    print(sent)\n",
    "    print(sentence_ners[idx])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50308"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 13.,  68., 147., 119.,  80.,  58.,  44.,  18.,  10.,  15.,   4.,\n",
       "          5.,   1.,   0.,   0.,   1.,   0.,   0.,   0.,   1.]),\n",
       " array([ 13. ,  30.8,  48.6,  66.4,  84.2, 102. , 119.8, 137.6, 155.4,\n",
       "        173.2, 191. , 208.8, 226.6, 244.4, 262.2, 280. , 297.8, 315.6,\n",
       "        333.4, 351.2, 369. ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "list_size=list()\n",
    "for elt in sentence_text:\n",
    "    list_size.append(len(elt))\n",
    "plt.hist(list_size, range = (min(list_size), max(list_size)), bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.97431506849315\n",
      "134.70000000000005\n",
      "164.70000000000005\n",
      "180.52999999999997\n",
      "209.33999999999992\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(list_size))\n",
    "print(np.quantile(list_size, 0.90))\n",
    "print(np.quantile(list_size, 0.95))\n",
    "print(np.quantile(list_size, 0.97))\n",
    "print(np.quantile(list_size, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get vocabulary and index inputs\n",
    "\n",
    "We will index each word from 1 according to inverse frequency (most common word is 1, etc.) until the max-vocab size. We will reserve two slots, 0 for the PAD index, and MAX_VOCAB-1 for out-of-vocabulary or unknown words (UNK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab: 3413\n"
     ]
    }
   ],
   "source": [
    "# text vocab dicts\n",
    "# subtract 2 for UNK, PAD\n",
    "word2idx, idx2word = get_vocab(sentence_text, MAX_VOCAB-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3414"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab: 45\n"
     ]
    }
   ],
   "source": [
    "# NER tag vocab dicts\n",
    "ner2idx, idx2ner = get_vocab(sentence_ners, len(set(nertags))+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "sentence_text_idx = index_sents(sentence_text, word2idx)\n",
    "sentence_ners_idx = index_sents(sentence_ners, ner2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-test splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(sentence_text))]\n",
    "\n",
    "train_idx, test_idx, y_train_ner, y_test_ner = train_test_split(indices, sentence_ners_idx, test_size=TEST_SIZE)\n",
    "\n",
    "def get_sublist(lst, indices):\n",
    "    result = []\n",
    "    for idx in indices:\n",
    "        result.append(lst[idx])\n",
    "    return result\n",
    "\n",
    "X_train_sents = get_sublist(sentence_text_idx, train_idx)\n",
    "X_test_sents = get_sublist(sentence_text_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network hyperparameters\n",
    "MAX_LENGTH = 170 # par rapport au graphique taille\n",
    "MAX_VOCAB = 5000    \n",
    "WORDEMBED_SIZE = 300 \n",
    "HIDDEN_SIZE = 400    # LSTM Nodes/Features/Dimension\n",
    "BATCH_SIZE = 64\n",
    "DROPOUTRATE = 0.25\n",
    "MAX_EPOCHS = 8       #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we must 'pad' our input and output sequences to a fixed length due to Tensorflow's fixed-graph representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero-padding sequences...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# zero-pad the sequences to max length\n",
    "print(\"zero-padding sequences...\\n\")\n",
    "X_train_sents = sequence.pad_sequences(X_train_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_test_sents = sequence.pad_sequences(X_test_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "y_train_ner = sequence.pad_sequences(y_train_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "y_test_ner = sequence.pad_sequences(y_test_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  43,  128,  152,   63,  211,  138,   24,  392,   11,  108,   27,\n",
       "         15,   23,   15,   19,   15,   31,   35,   15,   12,  117,   40,\n",
       "         15,   10,    2,  118,   15,   13,   88,   15,  199,    4,  131,\n",
       "         15,    7,   37,    4,    5,   46,   11,   66,    3,   58,   64,\n",
       "          4,  115,   16,  410,   17,  140,    1,  119,   18,    3,   50,\n",
       "       3228,  225,  194,    3,   94,    2,  100,    3,  116,  761,   16,\n",
       "         45,  352,    4,  141,  389,   71,  170,    2,  700,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of ner tags\n",
    "NER_VOCAB = len(list(idx2ner.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for CRF\n",
    "y_train_ner = y_train_ner[:, :, np.newaxis]\n",
    "y_test_ner = y_test_ner[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras_contrib\\layers\\crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras_contrib\\layers\\crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    }
   ],
   "source": [
    "# text layers \n",
    "txt_input = Input(shape=(MAX_LENGTH,), name='txt_input')\n",
    "txt_embed = Embedding(MAX_VOCAB, WORDEMBED_SIZE, input_length=MAX_LENGTH,\n",
    "                      name='txt_embedding', trainable=True, mask_zero=True)(txt_input)\n",
    "\n",
    "txt_drpot = Dropout(DROPOUTRATE, name='txt_dropout')(txt_embed)\n",
    "\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='mrg_bidirectional_1')(txt_drpot)#(mrg_cncat)\n",
    "\n",
    "# extra LSTM layer, if wanted\n",
    "mrg_drpot = Dropout(DROPOUTRATE, name='mrg_dropout')(mrg_lstml)\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='mrg_bidirectional_2')(mrg_lstml)\n",
    "\n",
    "# final linear chain CRF layer\n",
    "crf = CRF(NER_VOCAB, sparse_target=True)\n",
    "mrg_chain = crf(mrg_lstml)\n",
    "\n",
    "model = Model(inputs=[txt_input], outputs=mrg_chain)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=crf.loss_function,\n",
    "              metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "txt_input (InputLayer)       (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "txt_embedding (Embedding)    (None, 170, 300)          1500000   \n",
      "_________________________________________________________________\n",
      "txt_dropout (Dropout)        (None, 170, 300)          0         \n",
      "_________________________________________________________________\n",
      "mrg_bidirectional_1 (Bidirec (None, 170, 800)          2243200   \n",
      "_________________________________________________________________\n",
      "mrg_bidirectional_2 (Bidirec (None, 170, 800)          3843200   \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 170, 47)           39950     \n",
      "=================================================================\n",
      "Total params: 7,626,350\n",
      "Trainable params: 7,626,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/2\n",
      " - 163s - loss: 8.1182 - crf_viterbi_accuracy: 0.6147\n",
      "Epoch 2/2\n",
      " - 174s - loss: 6.5720 - crf_viterbi_accuracy: 0.8715\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_sents], y_train_ner,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS//4,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_test_sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 170)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis=-1)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 170)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trues = np.squeeze(y_test_ner, axis=-1)\n",
    "trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_preds = [[idx2ner[t] for t in s] for s in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_trues = [[idx2ner[t] for t in s] for s in trues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    from scrapinghub's python-crfsuite example\n",
    "    \n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O', 'PAD'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "               B-ADRESSE       0.00      0.00      0.00        38\n",
      "               I-ADRESSE       0.00      0.00      0.00        82\n",
      "    B-ANNEE_CONSTRUCTION       0.00      0.00      0.00        14\n",
      "    I-ANNEE_CONSTRUCTION       0.00      0.00      0.00         6\n",
      "        B-AVEC_ASCENSEUR       0.00      0.00      0.00        33\n",
      "        I-AVEC_ASCENSEUR       0.00      0.00      0.00        24\n",
      "B-CHARGES_LOCATAIRE_MOIS       0.00      0.00      0.00        18\n",
      "           B-CODE_POSTAL       0.00      0.00      0.00         7\n",
      "           B-COPROPRIETE       0.00      0.00      0.00        14\n",
      "            B-DATE_DISPO       0.00      0.00      0.00        36\n",
      "            I-DATE_DISPO       0.00      0.00      0.00        28\n",
      "        B-DEPOT_GARANTIE       0.00      0.00      0.00        13\n",
      "        I-DEPOT_GARANTIE       0.00      0.00      0.00         5\n",
      "             B-EXTERIEUR       0.00      0.00      0.00        60\n",
      "             B-HONORAIRE       0.00      0.00      0.00        48\n",
      "              B-LOYER_CC       0.00      0.00      0.00        19\n",
      "              I-LOYER_CC       0.00      0.00      0.00         1\n",
      "              B-LOYER_HC       0.00      0.00      0.00         6\n",
      "                    B-M2       0.00      0.00      0.00        87\n",
      "            B-N_CHAMBRES       0.00      0.00      0.00        70\n",
      "               B-N_ETAGE       0.00      0.00      0.00        67\n",
      "              B-N_PIECES       0.00      0.00      0.00        79\n",
      "               B-PARKING       0.00      0.00      0.00        51\n",
      "              B-QUARTIER       0.00      0.00      0.00        26\n",
      "              I-QUARTIER       0.00      0.00      0.00        27\n",
      "              B-STOCKAGE       0.00      0.00      0.00        47\n",
      "  B-TRANSPORTS_PROXIMITE       0.00      0.00      0.00        83\n",
      "  I-TRANSPORTS_PROXIMITE       0.00      0.00      0.00        42\n",
      "        B-TYPE_CHAUFFAGE       0.00      0.00      0.00        46\n",
      "         B-TYPE_LOCATION       0.00      0.00      0.00        28\n",
      "         I-TYPE_LOCATION       0.00      0.00      0.00         1\n",
      "                 B-VILLE       0.00      0.00      0.00        54\n",
      "                 I-VILLE       0.00      0.00      0.00        16\n",
      "\n",
      "               micro avg       0.00      0.00      0.00      1176\n",
      "               macro avg       0.00      0.00      0.00      1176\n",
      "            weighted avg       0.00      0.00      0.00      1176\n",
      "             samples avg       0.00      0.00      0.00      1176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\sujiv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(s_trues, s_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for sent_idx in range(len(X_test_sents[:500])):\n",
    "    \n",
    "    this_txt = sequence.pad_sequences([X_test_sents[sent_idx]], maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "    this_pred = model.predict([this_txt])\n",
    "    this_pred = [np.argmax(p) for p in this_pred[0]]\n",
    "    np.shape(this_pred)\n",
    "\n",
    "    word, tru, prd = [], [], []\n",
    "\n",
    "    # for each word in the sentence...\n",
    "    for idx, wordid in enumerate(X_test_sents[sent_idx][:len(this_pred)]):\n",
    "\n",
    "        # decode word\n",
    "        word.append(idx2word[wordid])\n",
    "        # decode true NER tag\n",
    "        tru.append(idx2ner[int(y_test_ner[sent_idx][idx])])\n",
    "        # decode prediction\n",
    "        prd.append(idx2ner[this_pred[idx]])\n",
    "\n",
    "    answ = pd.DataFrame(\n",
    "    {\n",
    "        'word': word,\n",
    "        'true': tru,\n",
    "        'pred': prd,\n",
    "        'skip' : [' ' for s in word]\n",
    "    })\n",
    "    answ = answ[['word', 'true', 'pred', 'skip']]\n",
    "    answ = answ.T\n",
    "    decoded.append(answ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>word</td>\n",
       "      <td>``</td>\n",
       "      <td>colocation</td>\n",
       "      <td>acceptée</td>\n",
       "      <td>-</td>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "      <td>min</td>\n",
       "      <td>de</td>\n",
       "      <td>la</td>\n",
       "      <td>gare</td>\n",
       "      <td>...</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-TRANSPORTS_PROXIMITE</td>\n",
       "      <td>...</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pred</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>skip</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word</td>\n",
       "      <td>nous</td>\n",
       "      <td>vous</td>\n",
       "      <td>proposons</td>\n",
       "      <td>un</td>\n",
       "      <td>appartement</td>\n",
       "      <td>de</td>\n",
       "      <td>3</td>\n",
       "      <td>pièces</td>\n",
       "      <td>situé</td>\n",
       "      <td>à</td>\n",
       "      <td>...</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>skip</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>word</td>\n",
       "      <td>a</td>\n",
       "      <td>10</td>\n",
       "      <td>minutes</td>\n",
       "      <td>en</td>\n",
       "      <td>bus</td>\n",
       "      <td>du</td>\n",
       "      <td>rer</td>\n",
       "      <td>b</td>\n",
       "      <td>robinson</td>\n",
       "      <td>et</td>\n",
       "      <td>...</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-TRANSPORTS_PROXIMITE</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pred</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "      <td>PAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>skip</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0           1          2   3            4   5    \\\n",
       "word    ``  colocation   acceptée   -            a   5   \n",
       "true     O           O          O   O            O   O   \n",
       "pred     O           O          O   O            O   O   \n",
       "skip                                                     \n",
       "word  nous        vous  proposons  un  appartement  de   \n",
       "...    ...         ...        ...  ..          ...  ..   \n",
       "skip                                                     \n",
       "word     a          10    minutes  en          bus  du   \n",
       "true     O           O          O   O            O   O   \n",
       "pred     O           O          O   O            O   O   \n",
       "skip                                                     \n",
       "\n",
       "                         6       7         8                       9    ...  \\\n",
       "word                     min      de        la                    gare  ...   \n",
       "true                       O       O         O  B-TRANSPORTS_PROXIMITE  ...   \n",
       "pred                       O       O         O                       O  ...   \n",
       "skip                                                                    ...   \n",
       "word                       3  pièces     situé                       à  ...   \n",
       "...                      ...     ...       ...                     ...  ...   \n",
       "skip                                                                    ...   \n",
       "word                     rer       b  robinson                      et  ...   \n",
       "true  B-TRANSPORTS_PROXIMITE       O         O                       O  ...   \n",
       "pred                       O       O         O                       O  ...   \n",
       "skip                                                                    ...   \n",
       "\n",
       "      160  161  162  163  164  165  166  167  168  169  \n",
       "word  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  \n",
       "true  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  \n",
       "pred  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  \n",
       "skip                                                    \n",
       "word  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "skip                                                    \n",
       "word  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  \n",
       "true  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  \n",
       "pred  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  PAD  \n",
       "skip                                                    \n",
       "\n",
       "[468 rows x 170 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat(decoded)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
